---
title: Car Insurance Claims Classification
authors:
    name: Hamed Vaheb
    email: hamed.vaheb.001@student.uni.lu
abstract: 
  This report describes the project of analysis and prediction of the car insurance claims dataset. The dataset includes historical data of the policyholders (e.g., age, gender, vehicle details, etc.), among which the variable of interest is the outcome of insurance, indicating whether a customer's claim is approved or not. The `aiinsurance` R package \cite{package} is developed to make this work reproducible, accessible, and equipped with advanced features, e.g., a pipeline that performs the main steps of this work in a single command, and an interactive app that displays the performance of the classification models, logistic regression and random forest, which are implemented using the package's functions and their purposer was to classify the outcomes based on solely the historical data. The results indicate promising prediction performance with various evaluation metrics of over 70 percent. In addition to classification, informative insights from the dataset and models have been drawn, e.g., statistical signifcance of the variables playing role in prediction.

bibliography: references.bib
biblio-style: unsrt
output: rticles::arxiv_article
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(aiinsurance)
library(dplyr)
library(janitor)
library(visdat)
library(ggplot2)
library(imbalance)
library(readr)
library(caret) #for dummyVar function
library(Metrics)
library(randomForest)
library(tidyverse)
library(fontawesome)
library(gridExtra)
library(kableExtra)
#knitr::opts_chunk$set(fig.width=3, fig.height=3)
```

# Introduction
\begin{wrapfigure}{l}{0.35\textwidth}
\includegraphics[width=0.9\linewidth]{./figures/aiactuary.png}
\end{wrapfigure}

Rapid advances in artificial intelligence (AI) and machine learning are creating products and services with
the potential not only to change the environment in which actuaries operate but also to provide new opportunities within actuarial science \cite{aiinins}.

The use of statistical learning models has been a common practice in actuarial science since the 1980s. 
It was not long after since the field adopted classical models, such as linear models and generalized linear models (GLMs).
While actuaries use GLMs frequently in practice, it was in the past few years that the use of AI and machine learning, and hence more modern models garnered significant attention in the field \cite{rev1}.

The goal of this work is to predict status of policyholders' claims. 
The status lies at the "outcome" column of the car insruance dataset, indicating whether a customer has claimed his loan or not.

A classical linear model and a modern nonlinear model is used and compared.
The former model is logistic regression, which is an example of GLMs, accommodated to classification setting, i.e., for predicting discrete classes, in this work's case, status of claims.
The latter model is a nonlinear tree-based model.

The remainder of this work is organized as the following:
In section \ref{sec:concepts}, main concepts included in the work are explained. In \ref{subsec:claim}, the concept of car insurance is introduced followed by an elaboration on claims and how they interact among an insurer and a policyholder. Subsequently, the two machine learning models, logistic regression \ref{subsec:model-logit}, and random forest \ref{subsec:model-rf} are briefly explained.
As a contribution of this work, the `aiinsurance` package \cite{package}, introduced in \ref{sec:package}, is developed, which automates many of the classification tasks, and uses advanced features, e.g., pipeline and interactive app.
In \ref{sec:data}, data is explored with a focus on expressing the status of categorical variables as well as missing values, both of which will be treated in \ref{sec:process}. which contains all the preprocessing steps. These steps are required before fitting the two classification models used, random forest and logit, their provided results in \ref{sec:modelling}. Finally, the models' performance are evaluated in \ref{sec:evaluation}.

\newpage

# Preliminary Concepts \label{sec:concepts}
## Car Insurance Claims \label{subsec:claim}
\begin{wrapfigure}{l}{0.25\textwidth}
\includegraphics[width=0.9\linewidth]{./figures/carinsurance.png}
\end{wrapfigure}

Car insurance is a type of insurance policy that financially protects drivers in the event of an accident or theft. There are several types of car insurance coverage, including liability insurance, collision insurance, comprehensive insurance, and personal injury protection. 

An insurance claim is a formal request by a policyholder to an insurance company for coverage or compensation for a covered loss or policy event, in this work's case, a car accident. The insurance company either accepts or rejects the claim. If it is approved, the insurance company will issue payment to the insured or an approved interested party on behalf of the insured.

Predicting the outcome of claims can be utilized to better understand the customer strata and incorporate the findings throughout the insurance policy enrollment (including the underwriting and approval or rejection stages), triage claims and automate where possible, gradually obviating the need for human interaction, and optimize the entire insurance policy enrollment process flow \cite{claim}.

## Model: Logistic Regression \label{subsec:model-logit}
\begin{wrapfigure}{l}{0.2\textwidth}
\includegraphics[width=0.9\linewidth]{./figures/logit-model.png}
\end{wrapfigure}

Logistic regression (logit), which is type of GLM, uses the logistic function to model the probability of the binary outcome by creating a model that takes in the input variable (e.g., client's information) and produces a probability that a  "outcome" (the variable we aim to predict) is 1. The "outcome" in this work is whether a customer's claim is approved or not.
The logistic function is defined as the following:

$$\frac{1}{1 + e^{-x}}$$

This probability can then be used to make a prediction about the outcome. For instance, if the probability that the client's claim will be approved is greater than a certain threshold (e.g., 0.5), we predict that the claim will be approved.
In another words, the goal of logit is to find the best set of coefficients for a set of independent variables that maximizes the likelihood (measuring how well parameters of a model fit the data) of the observed data.

## Model: Random Forest \label{subsec:model-rf}
\begin{wrapfigure}{l}{0.35\textwidth}
\includegraphics[width=0.9\linewidth]{./figures/rf-model.png}
\end{wrapfigure}

An ensemble learning model is a model that is constructed from multiple models, to obtain combined results, expected to be improved compared to any of the constituent models. The random forest is an ensemble model built from decision trees, i.e., flowchart-like tree structures, wherein each internal node represents a "test" on a variable (e.g., vehicle type), each branch represents the outcome of the test, and each leaf node represents a class label (e.g., 1 for claim approval and 0 for claim rejection). The intuition behind a decision tree is to recursively split the data into subsets based on the values of the input variables, such that the subsets are as "pure" as possible in terms of their class labels. The less pure a subset is, the more the data belongs to the same class, and the more pure it is, the more data is evenly split among all classes. The goal is to create a tree that can accurately classify new examples by traversing the tree from the root to a leaf node, making decisions at each internal node based on the values of the input variables.

As a single random tree might not be able to capture the proper inherent complexity of a data, and may either overfit (become too complex and remembers data rather than learning from it) or underfit (become too simple and hence unable to learn the inherent complicated patterns in the data). By training multiple decision trees and combining their predictions by taking a majority vote, a random forest is able to capture a more robust and accurate representation of the data. The randomness in the random forest comes from randomly selecting subsets of the data to train each decision tree, and randomly selecting subsets of input variables to consider at each split point in the decision tree. This helps to decorrelate the trees, i.e., reducing the correlation or dependence between the trees, consequently, make the model more robust to overfitting, i.e., becoming too complex and remembers data rather than learning from it.

# `aiinsurance` Package \label{sec:package} 
\begin{wrapfigure}{l}{0.15\textwidth}
\includegraphics[width=0.9\linewidth]{"./figures/logo.png"}
\end{wrapfigure}

The `aiinsurance` R package \cite{package} is developed to make this work reproducible, accessible, and equipped with advanced features.

Instructions on how to install and use the package's functions and features is provided in the [README part of package's Github repository](https://github.com/berserkhmdvhb/aiinsurance#readme) `r fa(name = "github")`.

Noteworthy features of the package are explained in the following: 

1. Multiple functions that facilitated and automated processes in \ref{sec:process} and \ref{subsec:missing}. The functions are well documented and numerous unit tests are conducted on them. This package was aimed to be developed as generalizable as possible, so that hopefully we little effort, it can be used for other datasets with classification setting.

2. Both raw dataset and processed train and tests (explained in \ref{subsec:split}) are incorporated in the package.

3. A pipeline that performs the main steps of this work in a single command. The pipeline takes the processed train and test datsets and output the evaluation plots in \ref{sec:evaluation}.

4. An interactive shiny app after performing the same tasks in pipeline, displays the evaluation plots in \ref{sec:evaluation} based on the type of the model and the type of the plot that user perfers.

# Exploratory Data Analysis (EDA) \label{sec:data}
An overview of the car insurance dataset is provided in the following:
\tiny
```{r, echo=FALSE, warning=FALSE}
data(car_insurance_data)
df <- janitor::clean_names(car_insurance_data)
df |> dplyr::glimpse()
```
\normalsize

The dataset has 19 variables (columns) and 10,000 observations (rows).
```{r, include=FALSE, warning=FALSE}
categoricals_print_hmd(df)
df <- categoricals_hmd(df) 
```

The following plot visualizes proportion of data types within the dataset.
```{r, echo=FALSE, warning=FALSE, fig.dim = c(4.3, 3), fig.align = 'center'}
options(repr.plot.width=2, repr.plot.height=2)
vis_dat(df) + ggplot2::theme(text = ggplot2::element_text(size = 9), ggplot2::element_line(size =1), plot.margin = unit(c(1,1,1,1),"cm"))
```

The `factor` types of the plot represent categorical variables containing classes as characters, e.g., `gender` variable contains characters `male` or `female`.
However, there are some categorical variables in the integer type as well, e.g., `married` which has values {1,0}, corresponding to being married or not respectively. The categorical columns should be treated and mapped to proper numeric representations, elaborated in \ref{subsec:encode}.

In the following, the proportion and location of missing values are visualized.

```{r, echo=FALSE, warning=FALSE, fig.dim = c(4, 3.5), fig.align = 'center'}

df[is.na(df) | df=="Inf"] = NA
options(repr.plot.width=2, repr.plot.height=2)
vis_miss(df, sort_miss = TRUE) + ggplot2::theme(text = ggplot2::element_text(size = 9), ggplot2::element_line(size =1), plot.margin = unit(c(1,1,1,1),"cm"))
```
Evidence by the plot, the columns containing missing values are 

- `credit_score`
- `annual_mileage`

The reason that the dataset is better not to contain missing values are explained in the following:

1. Many machine learning algorithms cannot handle missing values.
2. Missing values introduce bias or lead to inaccurate conclusions if the missing data is not missing at random. 
3. Missing values increase the variance of the estimate obtained from the data, which can lead to overfitting (i.e., becoming too complex and remembers data rather than learning from it).

Treating missing values is expalined in detail in \ref{subsec:missing}.

# Preprocessing \label{sec:process}

## Missing values \label{subsec:missing}

To proceed the processing and modeling parts, missing values of each variable are imputed with median for that particular variable. The reason that median is chosen for 
The reason missing values are better be removed are the following:


To fill the missing values, one must exercise prudence.
Imputing the missing values in a variable with some statistical property of that variable might lead to unwanted changes in nature of the variable, e.g., its distribution.

To observe the changes in imputed variables, the density plot of the variables `credit_score` and `annual_mileage` alongside their imputed versions. The dashed blue lines indicate the imputed versions.

```{r, include=FALSE, warning=FALSE}
df_copy <- df
df <- impute_median_hmd(df)

df_density <- data.frame(credit_score = df_copy$credit_score, 
                         credit_score_imputed = df$credit_score, 
                         annual_mileage = df_copy$annual_mileage, 
                         annual_mileage_imputed = df$annual_mileage)
df_density <- dplyr::mutate_all(df_density, function(x) as.numeric(x))
```


```{r, include=FALSE, warning=FALSE}
options(repr.plot.width=2, repr.plot.height=2)
p1 <- df_density |> 
  ggplot() +
  geom_density(aes(x = annual_mileage_imputed), alpha=0.1) +
  geom_density(aes(x = annual_mileage), color="blue", linetype="dashed", na.rm = TRUE, alpha=0.1) 

p2 <- df_density |> 
  ggplot() +
  geom_density(aes(x = credit_score_imputed), alpha=0.1) +
  geom_density(aes(x = credit_score), color="blue", linetype="dashed", na.rm = TRUE, alpha=0.1) 

p1 <- p1 + labs(caption = "Density plot of annual_mileage") + 
  ggplot2::theme(text = ggplot2::element_text(size = 9), ggplot2::element_line(size =1), plot.margin = unit(c(1,1,1,1),"cm")) +  theme(plot.caption = element_text(hjust=0.5))
p2 <- p2 +  labs(caption = "Density plot of credit_score") +
    ggplot2::theme(text = ggplot2::element_text(size = 9), ggplot2::element_line(size =1), plot.margin = unit(c(1,1,1,1),"cm")) + theme(plot.caption = element_text(hjust=0.5))

```

```{r, echo=FALSE, warning=FALSE, fig.dim = c(6,3.1),fig.align = 'center'}
gridExtra::grid.arrange(p1, p2, ncol = 2)
```

Evidenced by the plots, the `credit_score` variable's imputed version has a slight shift in its mean. 
Notwithstanding, this work resorted to median imputed versions, due to good performance of the models and due to computational intensity of more advanced methods, of which a prominent one is multiple imputation using chained equations \cite{mice}.


## Class Imabalance \label{subsec:imbalance}
The proportion of outcome classes are shown in the following:

```{r, echo=FALSE, fig.dim = c(2, 2.2), fig.align = 'center', warning=FALSE}
options(repr.plot.width=2, repr.plot.height=2)
ggplot(df, aes(x = outcome)) +
    geom_bar() +labs(caption = "Proportion of outcome classes") +
    ggplot2::theme(text = ggplot2::element_text(size = 9), ggplot2::element_line(size =1), plot.margin = unit(c(1,1,1,1),"cm")) + theme(plot.caption = element_text(hjust=0.5))
```

The number of class 0 and class 1 labels are respectively 6867, 3133.
Imbalance ratio is defined as size of minority class (1 in our case) over size of majority class (0). Therefore, this quantity is 1 for a fully balanced binary variable. Imbalance ratio for the outcome variable is 0.45624

To make the dataset balanced, oversampling methods can be used, which are methods that generate samples from the dataset with the minority class (having outcome = 1). The key challenge in these methods is that the samples should be similar to the original dataset, as their information, i.e., distribution and other statistical properties should align with the original data, however, they should be different to a small extent too, so that they would resemble new data available in the dataset, not merely a copy of what exists in it. 
A promising oversampling method that is used for this work is the RaCog algorithm. 
After oversampling using the mentioned method, the number of class 0 and class 1 labels would become respectively 5484, 5516, leading to a imbalance ratio approximating 1.
Prior to explaining the algorithm in \ref{subsec:racog}, the process of respresenting the categorical columns of the dataset as numerical ones is explained in \ref{subsec:encode}, as this process is a prerequisite for many oversampling methods. For RaCog, the dataset should be discretized and numeric, and a one-hot-encoded dataset satisfies this property.



## Encoding Categorical Columns \label{subsec:encode}
As mentioned earlier, a prerequisite of RaCog algorithm is a discretized and numeric dataset. In addition to this, there are other reasons for encoding the dataset to only contain numbers. Firstly, most machine learning algorithms require that input and output variables are numbers. Secondly, even if the dataset only contain numbers, and yet a categorical variable is represented with natural numbers, the following two cases might occur: Either the variable has a natural order, e.g., `age` which contains four age groups can be represented with numbers {1,2,3,4}, such that higher numbers correspond to older age group. However, the variables `vehicle_type`, `gender`, `race`, etc are not suitable to be represented with natural numbers, as higher or lower numbers doest not indicate any inherent ordrer, and this might mislead a machine learning model.
One-hot-encoding is used to encode (represent) categorical variables as numerical values but not natural numbers. This is done by creating a new binary column for each unique category in the data, with a value of 1 in the column corresponding to the category and 0 in all other columns.
One-hot-encoding wouldn't contain any order for classes of columns, and therefore overcomes the ordering issue mentioned earlier.

```{r, include=FALSE, warning=FALSE}
dummy <- dummyVars(" ~ .", data=df)
df_enc <- data.frame(predict(dummy, newdata = df)) 
dplyr::glimpse(df_enc)
df_dict <- train_test_splitter_hmd(df_enc, 
                             proportion=0.8)
train <- df_dict$train
test <- df_dict$test
for (col in names(train)){
  if (col %in% c("annual_mileage", "postal_code", "credit_score"))
  {
    next
  }
  train[[col]] <- as.integer(train[[col]])
  test[[col]] <- as.integer(test[[col]])
}
df_dict_norm <- normalizer_hmd(train,test)


train <- df_dict_norm$train_norm
test <- df_dict_norm$test_norm

train$id <- as.double(train$id)
test$id <- as.double(test$id)
```


## Train/Test Split \label{subsec:split}
After encoding the dataset, it is splitted to train and test datasets, with 80% proportion for the former and 20% for the latter.
The oversampling method in \ref{subsec:racog}, the classification models in \ref{sec:modelling}, and the hyperparameter tuning in \ref{subsec:hyperpar} are all done using the train data, and the evaluation processes in \ref{sec:evaluation} is done on the test data, which is the data that is unobserved by the models.

## RaCog \label{subsec:racog}
Existing oversampling approaches for addressing imbalanced dataset (explained in \ref{subsec:imbalance}) typically do not consider the probability distribution of the minority class while synthetically generating new samples. This leads to poor representation of the minority class, and hence to poor classification performance.

Rapidly converging Gibbs algorithm (RaCog) uses the joint probability distribution of input variables as well as Gibbs sampling to generate new minority class samples.  Gibbs sampling is a Markov Chain Monte Carlo (MCMC) method \cite{bishop} that is used to sample from a multi-dimensional distribution. The basic idea behind Gibbs sampling is that in order to sample from a probability distribution with multiple random variables, instead of directly sampling from the joint distribution of all the variables, the algorithm iteratively sample from the conditional distributions of each variable given the current values of the other variables. By dint of this, the computational difficulties of sampling from the full joint distribution are avoided. The result of the algorithm is a sequence of samples from the joint distribution of all the variables, which can be used to estimate various properties of the distribution, such as the mean and variance of each variable.

In summary, Gibbs sampling is a way to iteratively sample from the conditional distributions of each variable given the current values of the other variables, to estimate properties of a multi-dimensional distribution. For more rigorous and detailed information on RaCog alogirhtm, the interested reader is reffered to \cite{racog}.

In following, a grid of one to one variable comparison is presented, wherein the the prior imbalanced dataset in placed graphics next to the balanced one, for each pair of variables.

```{r, eval=FALSE, echo=FALSE, fig.dim = c(5.5,4), warning=FALSE}
tr <- within(train, rm("id"))
tr$outcome <- as.factor(tr$outcome)

tr2 <- insurance_train
tr2$outcome <- as.factor(tr2$outcome)

options(repr.plot.width=2, repr.plot.height=2)
pc <- imbalance::plotComparison(dataset = tr, anotherDataset = tr2, attrs = c("credit_score", "annual_mileage", "speeding_violations"), classAttr = "outcome") 
```

\begin{figure}[H] 
    \centerline{\includegraphics[width=9cm, height=7cm]{./figures/plotcompare.png}}
\end{figure}


# Classification Models \label{sec:modelling}
## Logistic Regression \label{subsec:logit}
When applying the logistic regression model (explained in)

```{r, include=FALSE, warning=FALSE}
data("insurance_train")
data("insurance_test")
actual <- insurance_test$outcome
```



```{r, include=FALSE, warning=FALSE}
actual <- insurance_test$outcome
fit <- glm_fit_hmd(insurance_train, target="outcome", family="binomial")
h <- glm_predict_hmd(fit, 
                        data = insurance_test,  
                        target = "outcome")
coef = h$coef
pred_glm <- h$predictions
pred_proba_glm <- h$predict_proba
eval_logit <- eval_hmd(actual, pred_glm)
accuracy_logit <- eval_logit$accuracy
precision_logit <- eval_logit$precision
recall_logit <- eval_logit$recall
```

Among results of fitting logistic regression (logit) on the dataset, the significance level of variables are reported, obtained by perturbing a small change in the variable under study while holding all other variables constant. The most significant variables that affects the prediction of outcomes, i.e., 1 for approval and 0 for or rejection of car insurance claims are listed below. The high significance levels were determined by observing small p-values. However, the direction of the variables' impact remains yet to be determined. The aim is to answer the following question. Would a significant have an effective role in increasing the chance of approval (class 1), or chance of rejection (class 0). The answer of this question lies at the sign of the coefficient of the variable in the model. 
The significant variables and their direction of significance are reported below. Positive effect means the variable increased chance in class 1.

- `driving_experience.0.9y`: Having driving experience between 0-9 years, positive.
- `vehicle_ownership`: Whether the policyholder owns the vehicle or not, negative.
- `vehicle_year.after.2015`: The vehicle was built after 2015, negative.
- `gender.female`: The policyholder being female, negative.
- `postal_code`: Postal code of the policyholder.



## Random Forest \label{subsec:rf}
The random forest model can also report significance level of variables using different measures, two of which will be reported, accuracy, and Gini. 

The following plot visualizes significance level of variables. The left part shows how much accuracy th model losses by excluding each variable. The more the accuracy suffers (higher `MeanDecreaseAccuracy`), the more important the variable is for the successful classification. The variables are presented in descending quantity order of importance.
The right part shows the mean decrease in Gini coefficient. The Gini coefficient is a measure of how each variable contributes to the homogeneity of the nodes and leaves in the resulting random forest. Again, higher `MeanDecreaseGini` indicates more significance.

```{r, include=FALSE, warning=FALSE}
rf <- rf_fit_hmd(insurance_train, 
                 ntree = 300,
                 mtry = 10,
                 proximity = TRUE, 
                 importance = TRUE)
predict_rf <- rf_predict_hmd(data=insurance_test, fit=rf)
eval_rf <- eval_hmd(actual,predict_rf$predictions_num)
pred_proba_rf <- predict_rf$predict_proba
```


```{r,eval=FALSE, include=FALSE, warning=FALSE}
eval_rf <- eval_logit
```


```{r, include=FALSE, warning=FALSE}
accuracy_rf <- eval_rf$accuracy
precision_rf <- eval_rf$precision
recall_rf <- eval_rf$recall
```

A salient point to take into account is that almost all variables for which both the logit model \ref{subsec:logit} considered high significance level also have high significance in random forest \ref{subsec:rf} based on at least one of the accuracy or Gini measurements. 
The only variable considered significant by logit but not by random forest was `gender.female`. 
And the variables that were considered significant by random forest but not by logit were the following:
`age`, `speeding_violations`, and `credit_score`.


```{r, eval=FALSE, include=FALSE, warning=FALSE, fig.dim = c(7.5,4), fig.align = 'center'}
options(repr.plot.width=2, repr.plot.height=2)
varImpPlot(rf, sort = T, n.var = 10, main = "Top 10 Variable Importance")  + ggplot2::theme(text = ggplot2::element_text(size = 9), ggplot2::element_line(size =1), plot.margin = unit(c(1,1,1,1),"cm"))
```
\begin{figure}[H] 
    \centerline{\includegraphics[width=8.2cm, height=5cm]{./figures/varimp.png}}
\end{figure}


## Hyper-parameter tuning \label{subsec:hyperpar}

Although the classification models and in general machine learning models learn to find the best parameters of in a given model, there are some parameters that are not learned during the training process, and they should be chosen by the human. Using intuition, tial and error, and relying on domain knowledge are common practices to find the best set of hyperparameterse. Another essential way is to do a systematic search and try all possible combinations. This can be computationally intensive. In this work, the following hyperparameters were chosen by search:

1. **Logit:**
  - $\lambda$: the parameter that controls the amount of                regularization applied to the modeling logistic regression          derived from generalized linear model.

2. **Random Forest:**
  - `ntree`: Number of trees
  - `mtry`: Number of variables randomly sampled as candidates at each split
# Evaluation \label{sec:evaluation}
In the following, various evaluation metrics employed to measure and compare the classification performance of the models are reported.
The test set (explained in \ref{subsec:split}) is used to evaluate the models. The classification of test set's outcome variable predicted by models are compared with the test set's real values of outcome. The evaluation metrics will be explained in the following:



## Confusion Matrix

The confusion matrix is made up of four different cells, each representing the number of correctly and incorrectly classified instances of a particular class. The cells are typically arranged in the following format:

```{r, echo=FALSE, warning=FALSE}
# Create a 2x2 matrix
mat <- matrix(c("True Positive (TP)", "False Negative (FN)", "False Positive (FP)", "True Negative (TN)"), nrow = 2, ncol = 2)


# Create a 2x2 matrix

# Create a table with lines between rows and columns
kable(mat, "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```
  
The four quantities are defined below:

- **True Positive (TP)**: cases where the model predicted the positive class, and the actual output was also positive.
- **False Positive (FP)**: cases where the model predicted the positive class, but the actual output was negative.
- **False Negative (FN)**: cases where the model predicted the negative class, but the actual output was positive.
- **True Negative (TN)**: cases where the model predicted the negative class, and the actual output was also negative.


In the following plot, the confusion matrix for both models are visualized.

```{r, echo=FALSE, warning=FALSE, fig.dim = c(6,2),  fig.align = 'center'}
p1 <- eval_logit$confusion_matrix_plot + labs(caption = "Confusion matrix of logit") +
  theme(plot.caption = element_text(hjust=0.5))
p2 <- eval_rf$confusion_matrix_plot +  labs(caption = "Confusion matrix of random forest") +
  theme(plot.caption = element_text(hjust=0.5))

gridExtra::grid.arrange(p1, p2, ncol = 2)
```

Both models demonstrate similar performance, with difference that logit have more misclassification cases for predicting class 0, and more corret classifications for class 1, whereas random forest have more misclassification cases for predicting class 1, and more corret classifications for class 0.

## Metrics derived from confusion matrix

The formulas for the evaluation metrics needed for the rest of the report are defined in the following, with terms that are from the confusion matrix explained earlier.
Two new terms used here are condition positive (P), i.e.,
the number of real positive cases in the data, and condition negative (N), i.e., the number of real negative cases in the data.

- Accuracy: $$ACC = \frac{TP + TN}{P + N} = \frac{TP + TN}{TP + TN + FP + FN}$$
- Precision (positive predictive value or PPV): $$PPV = \frac{TP}{TP + FP}$$
- FDR (false discovery rate): $$FDR = 1 - PPV$$
- Recall (True Positive Rate or TPR) $$TPR = \frac{TP}{P} = \frac{TP}{TP + FN}$$
- Fall-out (False Positive Rate or FPR): $$\frac{FP}{N} = \frac{FP}{FP + TN}$$ 

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Metric} & \textbf{Logit} & \textbf{Random Forest} \\
\hline
Accuracy & `r accuracy_logit` & `r accuracy_rf` \\
FDR & `r 1 - precision_logit` & `r 1 - precision_rf` \\
Precision & `r precision_logit` & `r precision_rf` \\
Recall & `r recall_logit` & `r recall_rf` \\
\hline
\end{tabular}
\caption{Comparison of Evaluation Metrics for Two Models}
\label{tab:comparison}
\end{table}

 From insurance company's perspective, the most prominent loss would be to approve a claim falsely. This is equivalent to misclassify an outcome to class 1 which originally belongs to class 0.
From the reported evaluation metrics, of particular note is the FDR (false discovery rate), as it shows the exact quantity discussed. 
The lower this quantity is, the less falsely approved claims will occur. This quantity is lower in logit, evidenced by \ref{tab:comparison}. Based on this, the logit is preferable.


## ROC Curve

An ROC curve plots the recall (true positive rate) against the false positive rate (FPR) at different classification thresholds. The curve is visualized for the two models in the following:



```{r, include=FALSE}
roc_logit <- pROC::roc(actual ~ pred_proba_glm, print.auc=FALSE)
roc_rf <- pROC::roc(actual ~ pred_proba_rf, print.auc=FALSE)
p1 <- pROC::ggroc(roc_logit)
p2 <- pROC::ggroc(roc_rf)
```





```{r, echo=FALSE, warning=FALSE, fig.dim = c(5,2.5), fig.align = 'center'}
options(repr.plot.width=2, repr.plot.height=2)
p1 <- p1 + labs(caption = "ROC curve of logit") + 
  ggplot2::theme(text = ggplot2::element_text(size = 9), ggplot2::element_line(size =1), plot.margin = unit(c(1,1,1,1),"cm")) + 
  theme(plot.caption = element_text(hjust=0.5))
p2 <- p2 +  labs(caption = "ROC curve of random forest") +
    ggplot2::theme(text = ggplot2::element_text(size = 9), ggplot2::element_line(size =1), plot.margin = unit(c(1,1,1,1),"cm")) +
  theme(plot.caption = element_text(hjust=0.5))

gridExtra::grid.arrange(p1, p2, ncol = 2)
```

# Conclusion

This report described the project of analysis of the car insurance claims dataset, and prediction of claims' outcomes using random forest and logistic regression. Both models had promising performance, with most evaluation metrics more than 70 percent.
Since the insurance company would be more concerned about falsely approving a claim, the false discovery rate metrics was reported as well, and logit had lower false discovery rate, making it a more suitable candidate. The `aiinsurance` R package \cite{package} is developed to make this work reproducible, accessible, and equipped with advanced features, e.g., a pipeline that performs the main steps of this work in a single command, and an interactive app that displays the performance of the models. Based on judgement of both models, the following variables had the most effective role in prediction, i.e., in the policyholder's claim more likely being accepted:
Having driving experience between 0-9 years would make the claim more likely to accept. On the other hand, not owning the vehicle, the vehicle being built after 2015, and the policyholder being female would make the claim more likely to be rejected.

