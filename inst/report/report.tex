\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{lmodern}        % https://github.com/rstudio/rticles/issues/343
%[HMD] change reference style
\usepackage{xcolor,sectsty}
\definecolor{astral}{RGB}{46,116,181}
\definecolor{capri}{rgb}{0.0, 0.75, 1.0}
\usepackage[pagebackref=false,colorlinks,urlcolor=capri,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{caption}



%[HMD] change the font
%For SANS-SERIF
%\usepackage{biolinum}
%\renewcommand{\familydefault}{\sfdefault}
% For SERIF
\usepackage{libertine}
\usepackage{libertinust1math}

%[HMD] change title color
%\title{Car Insurance Claims Classification}
\title{\color{astral}Car Insurance Claims Classification}

\author{
    Hamed Vaheb\\
  \texttt{\href{mailto:hamed.vaheb.001@student.uni.lu}{\nolinkurl{hamed.vaheb.001@student.uni.lu}}}% \\
  }


% tightlist command for lists without linebreak
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



\begin{document}
\maketitle


\begin{abstract}
This report describes the project of analysis and prediction of the car
insurance claims dataset. The dataset includes historical data of the
policyholders (e.g., age, gender, vehicle details, etc.), among which
the variable of interest is the outcome of insurance, indicating whether
a customer's claim is approved or not. The \texttt{aiinsurance} R
package \cite{package} is developed to make this work reproducible,
accessible, and equipped with advanced features, e.g., a pipeline that
performs the main steps of this work in a single command, and an
interactive app that displays the performance of the classification
models, logistic regression and random forest, which are implemented
using the package's functions and their purposer was to classify the
outcomes based on solely the historical data. The results indicate
promising prediction performance with various evaluation metrics of over
80 percent. In addition to classification, informative insights from the
dataset and models have been drawn, e.g., statistical signifcance of the
variables playing role in prediction.
\end{abstract}


\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\begin{wrapfigure}{l}{0.35\textwidth}
\includegraphics[width=0.9\linewidth]{./figures/aiactuary.png}
\end{wrapfigure}

Rapid advances in artificial intelligence (AI) and machine learning are
creating products and services with the potential not only to change the
environment in which actuaries operate but also to provide new
opportunities within actuarial science \cite{aiinins}.

The use of statistical learning models has been a common practice in
actuarial science since the 1980s. It was not long after since the field
adopted classical models, such as linear models and generalized linear
models (GLMs). While actuaries use GLMs frequently in practice, it was
in the past few years that the use of AI and machine learning, and hence
more modern models garnered significant attention in the field
\cite{rev1}.

The goal of this work is to predict status of policyholders' claims. The
status lies at the ``outcome'' column of the car insruance dataset,
indicating whether a customer has claimed his loan or not.

A classical linear model and a modern nonlinear model is used and
compared. The former model is logistic regression, which is an example
of GLMs, accommodated to classification setting, i.e., for predicting
discrete classes, in this work's case, status of claims. The latter
model is a nonlinear tree-based model.

The remainder of this work is organized as the following: In section
\ref{sec:concepts}, main concepts included in the work are explained. In
\ref{subsec:claim}, the concept of car insurance is introduced followed
by an elaboration on claims and how they interact among an insurer and a
policyholder. Subsequently, the two machine learning models, logistic
regression \ref{subsec:model-logit}, and random forest
\ref{subsec:model-rf} are briefly explained. As a contribution of this
work, the \texttt{aiinsurance} package \cite{package}, introduced in
\ref{sec:package}, is developed, which automates many of the
classification tasks, and uses advanced features, e.g., pipeline and
interactive app. In \ref{sec:data}, data is explored with a focus on
expressing the status of categorical variables as well as missing
values, both of which will be treated in \ref{sec:process}. which
contains all the preprocessing steps. These steps are required before
fitting the two classification models used, random forest and logit,
their provided results in \ref{sec:modelling}. Finally, the models'
performance are evaluated in \ref{sec:evaluation}.

\newpage

\hypertarget{preliminary-concepts}{%
\section{\texorpdfstring{Preliminary Concepts
\label{sec:concepts}}{Preliminary Concepts }}\label{preliminary-concepts}}

\hypertarget{car-insurance-claims}{%
\subsection{\texorpdfstring{Car Insurance Claims
\label{subsec:claim}}{Car Insurance Claims }}\label{car-insurance-claims}}

\begin{wrapfigure}{l}{0.25\textwidth}
\includegraphics[width=0.9\linewidth]{./figures/carinsurance.png}
\end{wrapfigure}

Car insurance is a type of insurance policy that financially protects
drivers in the event of an accident or theft. There are several types of
car insurance coverage, including liability insurance, collision
insurance, comprehensive insurance, and personal injury protection.

An insurance claim is a formal request by a policyholder to an insurance
company for coverage or compensation for a covered loss or policy event,
in this work's case, a car accident. The insurance company either
accepts or rejects the claim. If it is approved, the insurance company
will issue payment to the insured or an approved interested party on
behalf of the insured.

Predicting the outcome of claims can be utilized to better understand
the customer strata and incorporate the findings throughout the
insurance policy enrollment (including the underwriting and approval or
rejection stages), triage claims and automate where possible, gradually
obviating the need for human interaction, and optimize the entire
insurance policy enrollment process flow \cite{claim}.

\hypertarget{model-logistic-regression}{%
\subsection{\texorpdfstring{Model: Logistic Regression
\label{subsec:model-logit}}{Model: Logistic Regression }}\label{model-logistic-regression}}

\begin{wrapfigure}{l}{0.2\textwidth}
\includegraphics[width=0.9\linewidth]{./figures/logit-model.png}
\end{wrapfigure}

Logistic regression (logit), which is type of GLM, uses the logistic
function to model the probability of the binary outcome by creating a
model that takes in the input variable (e.g., client's information) and
produces a probability that a ``outcome'' (the variable we aim to
predict) is 1. The ``outcome'' in this work is whether a customer's
claim is approved or not. The logistic function is defined as the
following:

\[\frac{1}{1 + e^{-x}}\]

This probability can then be used to make a prediction about the
outcome. For instance, if the probability that the client's claim will
be approved is greater than a certain threshold (e.g., 0.5), we predict
that the claim will be approved. In another words, the goal of logit is
to find the best set of coefficients for a set of independent variables
that maximizes the likelihood (measuring how well parameters of a model
fit the data) of the observed data.

\hypertarget{model-random-forest}{%
\subsection{\texorpdfstring{Model: Random Forest
\label{subsec:model-rf}}{Model: Random Forest }}\label{model-random-forest}}

\begin{wrapfigure}{l}{0.35\textwidth}
\includegraphics[width=0.9\linewidth]{./figures/rf-model.png}
\end{wrapfigure}

An ensemble learning model is a model that is constructed from multiple
models, to obtain combined results, expected to be improved compared to
any of the constituent models. The random forest is an ensemble model
built from decision trees, i.e., flowchart-like tree structures, wherein
each internal node represents a ``test'' on a variable (e.g., vehicle
type), each branch represents the outcome of the test, and each leaf
node represents a class label (e.g., 1 for claim approval and 0 for
claim rejection). The intuition behind a decision tree is to recursively
split the data into subsets based on the values of the input variables,
such that the subsets are as ``pure'' as possible in terms of their
class labels. The less pure a subset is, the more the data belongs to
the same class, and the more pure it is, the more data is evenly split
among all classes. The goal is to create a tree that can accurately
classify new examples by traversing the tree from the root to a leaf
node, making decisions at each internal node based on the values of the
input variables.

As a single random tree might not be able to capture the proper inherent
complexity of a data, and may either overfit (become too complex and
remembers data rather than learning from it) or underfit (become too
simple and hence unable to learn the inherent complicated patterns in
the data). By training multiple decision trees and combining their
predictions by taking a majority vote, a random forest is able to
capture a more robust and accurate representation of the data. The
randomness in the random forest comes from randomly selecting subsets of
the data to train each decision tree, and randomly selecting subsets of
input variables to consider at each split point in the decision tree.
This helps to decorrelate the trees, i.e., reducing the correlation or
dependence between the trees, consequently, make the model more robust
to overfitting, i.e., becoming too complex and remembers data rather
than learning from it.

\hypertarget{aiinsurance-package}{%
\section{\texorpdfstring{\texttt{aiinsurance} Package
\label{sec:package}}{aiinsurance Package }}\label{aiinsurance-package}}

\begin{wrapfigure}{l}{0.15\textwidth}
\includegraphics[width=0.9\linewidth]{"./figures/logo.png"}
\end{wrapfigure}

The \texttt{aiinsurance} R package \cite{package} is developed to make
this work reproducible, accessible, and equipped with advanced features.

Instructions on how to install and use the package's functions and
features is provided in the
\href{https://github.com/berserkhmdvhb/aiinsurance\#readme}{README part
of package's Github repository}
\includegraphics[width=0.97em,height=1em]{report_files/figure-latex/fa-icon-9e25601f72c0b4fff1c079a486ca8bba.pdf}.

Noteworthy features of the package are explained in the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Multiple functions that facilitated and automated processes in
  \ref{sec:process} and \ref{subsec:missing}. The functions are well
  documented and numerous unit tests are conducted on them.
\item
  Both raw dataset and processed train and tests (explained in
  \ref{subsec:split}) are incorporated in the package.
\item
  A pipeline that performs the main steps of this work in a single
  command. The pipeline takes the processed train and test datsets and
  output the evaluation plots in \ref{sec:evaluation}.
\item
  An interactive shiny app after performing the same tasks in pipeline,
  displays the evaluation plots in \ref{sec:evaluation} based on the
  type of the model and the type of the plot that user perfers.
\end{enumerate}

\hypertarget{exploratory-data-analysis-eda}{%
\section{\texorpdfstring{Exploratory Data Analysis (EDA)
\label{sec:data}}{Exploratory Data Analysis (EDA) }}\label{exploratory-data-analysis-eda}}

An overview of the car insurance dataset is provided in the following:
\tiny

\begin{verbatim}
## Rows: 10,000
## Columns: 19
## $ id                  <int> 569520, 750365, 199901, 478866, 731664, 877557, 93~
## $ age                 <chr> "65+", "16-25", "16-25", "16-25", "26-39", "40-64"~
## $ gender              <chr> "female", "male", "female", "male", "male", "femal~
## $ race                <chr> "majority", "majority", "majority", "majority", "m~
## $ driving_experience  <chr> "0-9y", "0-9y", "0-9y", "0-9y", "10-19y", "20-29y"~
## $ education           <chr> "high school", "none", "high school", "university"~
## $ income              <chr> "upper class", "poverty", "working class", "workin~
## $ credit_score        <dbl> 0.6290273, 0.3577571, 0.4931458, 0.2060129, 0.3883~
## $ vehicle_ownership   <int> 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,~
## $ vehicle_year        <chr> "after 2015", "before 2015", "before 2015", "befor~
## $ married             <int> 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,~
## $ children            <int> 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,~
## $ postal_code         <int> 10238, 10238, 10238, 32765, 32765, 10238, 10238, 1~
## $ annual_mileage      <int> 12000, 16000, 11000, 11000, 12000, 13000, 13000, 1~
## $ vehicle_type        <chr> "sedan", "sedan", "sedan", "sedan", "sedan", "seda~
## $ speeding_violations <int> 0, 0, 0, 0, 2, 3, 7, 0, 0, 0, 6, 4, 4, 0, 0, 0, 10~
## $ duis                <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 2, 0, 2,~
## $ past_accidents      <int> 0, 0, 0, 0, 1, 3, 3, 0, 0, 0, 7, 0, 2, 0, 1, 0, 1,~
## $ outcome             <int> 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,~
\end{verbatim}

\normalsize

The dataset has 19 variables (columns) and 10,000 observations (rows).

The following plot visualizes proportion of data types within the
dataset.

\begin{center}\includegraphics{report_files/figure-latex/unnamed-chunk-3-1} \end{center}

The \texttt{factor} types of the plot represent categorical variables
containing classes as characters, e.g., \texttt{gender} variable
contains characters \texttt{male} or \texttt{female}. However, there are
some categorical variables in the integer type as well, e.g.,
\texttt{married} which has values \{1,0\}, corresponding to being
married or not respectively. The categorical columns should be treated
and mapped to proper numeric representations, elaborated in
\ref{subsec:encode}.

In the following, the proportion and location of missing values are
visualized.

\begin{center}\includegraphics{report_files/figure-latex/unnamed-chunk-4-1} \end{center}

Evidence by the plot, the columns containing missing values are

\begin{itemize}
\tightlist
\item
  \texttt{credit\_score}
\item
  \texttt{annual\_mileage}
\end{itemize}

The reason that the dataset is better not to contain missing values are
explained in the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Many machine learning algorithms cannot handle missing values.
\item
  Missing values introduce bias or lead to inaccurate conclusions if the
  missing data is not missing at random.
\item
  Missing values increase the variance of the estimate obtained from the
  data, which can lead to overfitting (i.e., becoming too complex and
  remembers data rather than learning from it).
\end{enumerate}

Treating missing values is expalined in detail in \ref{subsec:missing}.

\hypertarget{preprocessing}{%
\section{\texorpdfstring{Preprocessing
\label{sec:process}}{Preprocessing }}\label{preprocessing}}

\hypertarget{missing-values}{%
\subsection{\texorpdfstring{Missing values
\label{subsec:missing}}{Missing values }}\label{missing-values}}

To proceed the processing and modeling parts, missing values of each
variable are imputed with median for that particular variable. The
reason that median is chosen for The reason missing values are better be
removed are the following:

To fill the missing values, one must exercise prudence. Imputing the
missing values in a variable with some statistical property of that
variable might lead to unwanted changes in nature of the variable, e.g.,
its distribution.

To observe the changes in imputed variables, the density plot of the
variables \texttt{credit\_score} and \texttt{annual\_mileage} alongside
their imputed versions. The dashed blue lines indicate the imputed
versions.

\begin{center}\includegraphics{report_files/figure-latex/unnamed-chunk-7-1} \end{center}

Evidenced by the plots, the \texttt{credit\_score} variable's imputed
version has a slight shift in its mean. Notwithstanding, this work
resorted to median imputed versions, due to good performance of the
models and due to computational intensity of more advanced methods, of
which a prominent one is multiple imputation using chained equations
\cite{mice}.

\hypertarget{class-imabalance}{%
\subsection{\texorpdfstring{Class Imabalance
\label{subsec:imbalance}}{Class Imabalance }}\label{class-imabalance}}

The proportion of outcome classes are shown in the following:

\begin{center}\includegraphics{report_files/figure-latex/unnamed-chunk-8-1} \end{center}

The number of class 0 and class 1 labels are respectively 6867, 3133.
Imbalance ratio is defined as size of minority class (1 in our case)
over size of majority class (0). Therefore, this quantity is 1 for a
fully balanced binary variable. Imbalance ratio for the outcome variable
is 0.45624

To make the dataset balanced, oversampling methods can be used, which
are methods that generate samples from the dataset with the minority
class (having outcome = 1). The key challenge in these methods is that
the samples should be similar to the original dataset, as their
information, i.e., distribution and other statistical properties should
align with the original data, however, they should be different to a
small extent too, so that they would resemble new data available in the
dataset, not merely a copy of what exists in it. A promising
oversampling method that is used for this work is the RaCog algorithm.
After oversampling using the mentioned method, the number of class 0 and
class 1 labels would become respectively 5484, 5516, leading to a
imbalance ratio approximating 1. Prior to explaining the algorithm in
\ref{subsec:racog}, the process of respresenting the categorical columns
of the dataset as numerical ones is explained in \ref{subsec:encode}, as
this process is a prerequisite for many oversampling methods. For RaCog,
the dataset should be discretized and numeric, and a one-hot-encoded
dataset satisfies this property.

\hypertarget{encoding-categorical-columns}{%
\subsection{\texorpdfstring{Encoding Categorical Columns
\label{subsec:encode}}{Encoding Categorical Columns }}\label{encoding-categorical-columns}}

As mentioned earlier, a prerequisite of RaCog algorithm is a discretized
and numeric dataset. In addition to this, there are other reasons for
encoding the dataset to only contain numbers. Firstly, most machine
learning algorithms require that input and output variables are numbers.
Secondly, even if the dataset only contain numbers, and yet a
categorical variable is represented with natural numbers, the following
two cases might occur: Either the variable has a natural order, e.g.,
\texttt{age} which contains four age groups can be represented with
numbers \{1,2,3,4\}, such that higher numbers correspond to older age
group. However, the variables \texttt{vehicle\_type}, \texttt{gender},
\texttt{race}, etc are not suitable to be represented with natural
numbers, as higher or lower numbers doest not indicate any inherent
ordrer, and this might mislead a machine learning model.
One-hot-encoding is used to encode (represent) categorical variables as
numerical values but not natural numbers. This is done by creating a new
binary column for each unique category in the data, with a value of 1 in
the column corresponding to the category and 0 in all other columns.
One-hot-encoding wouldn't contain any order for classes of columns, and
therefore overcomes the ordering issue mentioned earlier.

\hypertarget{traintest-split}{%
\subsection{\texorpdfstring{Train/Test Split
\label{subsec:split}}{Train/Test Split }}\label{traintest-split}}

After encoding the dataset, it is splitted to train and test datasets,
with 80\% proportion for the former and 20\% for the latter. The
oversampling method in \ref{subsec:racog}, the classification models in
\ref{sec:modelling}, and the hyperparameter tuning in
\ref{subsec:hyperpar} are all done using the train data, and the
evaluation processes in \ref{sec:evaluation} is done on the test data,
which is the data that is unobserved by the models.

\hypertarget{racog}{%
\subsection{\texorpdfstring{RaCog
\label{subsec:racog}}{RaCog }}\label{racog}}

Existing oversampling approaches for addressing imbalanced dataset
(explained in \ref{subsec:imbalance}) typically do not consider the
probability distribution of the minority class while synthetically
generating new samples. This leads to poor representation of the
minority class, and hence to poor classification performance.

Rapidly converging Gibbs algorithm (RaCog) uses the joint probability
distribution of input variables as well as Gibbs sampling to generate
new minority class samples. Gibbs sampling is a Markov Chain Monte Carlo
(MCMC) method \cite{bishop} that is used to sample from a
multi-dimensional distribution. The basic idea behind Gibbs sampling is
that in order to sample from a probability distribution with multiple
random variables, instead of directly sampling from the joint
distribution of all the variables, the algorithm iteratively sample from
the conditional distributions of each variable given the current values
of the other variables. By dint of this, the computational difficulties
of sampling from the full joint distribution are avoided. The result of
the algorithm is a sequence of samples from the joint distribution of
all the variables, which can be used to estimate various properties of
the distribution, such as the mean and variance of each variable.

In summary, Gibbs sampling is a way to iteratively sample from the
conditional distributions of each variable given the current values of
the other variables, to estimate properties of a multi-dimensional
distribution. For more rigorous and detailed information on RaCog
alogirhtm, the interested reader is reffered to \cite{racog}.

In following, a grid of one to one variable comparison is presented,
wherein the the prior imbalanced dataset in placed graphics next to the
balanced one, for each pair of variables.

\begin{figure}[htbp] 
    \centerline{\includegraphics[width=9cm, height=7cm]{./figures/plotcompare.png}}
\end{figure}

\hypertarget{classification-models}{%
\section{\texorpdfstring{Classification Models
\label{sec:modelling}}{Classification Models }}\label{classification-models}}

\hypertarget{logistic-regression}{%
\subsection{\texorpdfstring{Logistic Regression
\label{subsec:logit}}{Logistic Regression }}\label{logistic-regression}}

When applying the logistic regression model (explained in)

Among results of fitting logistic regression (logit) on the dataset, the
significance level of variables are reported, obtained by perturbing a
small change in the variable under study while holding all other
variables constant. The most significant variables that affects the
prediction of outcomes, i.e., 1 for approval and 0 for or rejection of
car insurance claims are listed below. The high significance levels were
determined by observing small p-values. However, the direction of the
variables' impact remains yet to be determined. The aim is to answer the
following question. Would a significant have an effective role in
increasing the chance of approval (class 1), or chance of rejection
(class 0). The answer of this question lies at the sign of the
coefficient of the variable in the model. The significant variables and
their direction of significance are reported below. Positive effect
means the variable increased chance in class 1.

\begin{itemize}
\tightlist
\item
  \texttt{driving\_experience.0.9y}: Having driving experience between
  0-9 years, positive.
\item
  \texttt{vehicle\_ownership}: Whether the policyholder owns the vehicle
  or not, negative.
\item
  \texttt{vehicle\_year.after.2015}: The vehicle was built after 2015,
  negative.
\item
  \texttt{postal\_code}: Postal code of the policyholder.
\end{itemize}

\hypertarget{random-forest}{%
\subsection{\texorpdfstring{Random Forest
\label{subsec:rf}}{Random Forest }}\label{random-forest}}

The random forest model can also report significance level of variables
using different measures, two of which will be reported, accuracy, and
Gini.

The following plot visualizes significance level of variables. The left
part shows how much accuracy th model losses by excluding each variable.
The more the accuracy suffers (higher \texttt{MeanDecreaseAccuracy}),
the more important the variable is for the successful classification.
The variables are presented in descending quantity order of importance.
The right part shows the mean decrease in Gini coefficient. The Gini
coefficient is a measure of how each variable contributes to the
homogeneity of the nodes and leaves in the resulting random forest.
Again, higher \texttt{MeanDecreaseGini} indicates more significance.

A salient point to take into account is that almost all variables for
which both the logit model \ref{subsec:logit} considered high
significance level also have high significance in random forest
\ref{subsec:rf} based on at least one of the accuracy or Gini
measurements. The only variable considered significant by logit but not
by random forest was \texttt{gender.female}. And the variables that were
considered significant by random forest but not by logit were the
following: \texttt{age}, \texttt{speeding\_violations}, and
\texttt{credit\_score}.

\begin{figure}[htbp] 
    \centerline{\includegraphics[width=8.2cm, height=5cm]{./figures/varimp.png}}
\end{figure}

\hypertarget{hyper-parameter-tuning}{%
\subsection{\texorpdfstring{Hyper-parameter tuning
\label{subsec:hyperpar}}{Hyper-parameter tuning }}\label{hyper-parameter-tuning}}

\hypertarget{evaluation}{%
\section{\texorpdfstring{Evaluation
\label{sec:evaluation}}{Evaluation }}\label{evaluation}}

In the following, various evaluation metrics were employed to measure
and compare the classification performance of the models.

\hypertarget{confusion-matrix}{%
\subsection{Confusion Matrix}\label{confusion-matrix}}

\begin{center}\includegraphics{report_files/figure-latex/unnamed-chunk-17-1} \end{center}

\hypertarget{accuracy-precision-recall-roc}{%
\subsection{Accuracy, Precision, Recall,
ROC}\label{accuracy-precision-recall-roc}}

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Metric} & \textbf{Logit} & \textbf{Random Forest} \\
\hline
Accuracy & 0.8115 & 0.8315 \\
Precision & 0.8444084 & 0.7990276 \\
Recall & 0.6496259 & 0.6983003 \\
\hline
\end{tabular}
\caption{Comparison of Evaluation Metrics for Two Models}
\label{tab:comparison}
\end{table}

\hypertarget{roc-curve}{%
\subsection{ROC Curve}\label{roc-curve}}

\begin{center}\includegraphics{report_files/figure-latex/unnamed-chunk-19-1} \end{center}

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

\bibliographystyle{unsrt}
\bibliography{references.bib}


\end{document}
